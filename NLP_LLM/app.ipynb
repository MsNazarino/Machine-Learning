{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486ee379",
   "metadata": {},
   "source": [
    "#### <center> <p style = 'background-color:darkblue'><b>Task - 1</p> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f2032aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb178cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–æ–∫–µ–Ω—ñ–∑–æ–≤–∞–Ω–∏–π —Ç–µ–∫—Å—Ç: ['Natural', 'language', 'processing', 'enables', 'machines', 'to', 'understand', 'human', 'language', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\gnatu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "texts = \"Natural language processing enables machines to understand human language.\"\n",
    "tokens = word_tokenize(texts)\n",
    "print(\"–¢–æ–∫–µ–Ω—ñ–∑–æ–≤–∞–Ω–∏–π —Ç–µ–∫—Å—Ç:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff0cd093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gnatu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gnatu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# üîπ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö —Ä–µ—Å—É—Ä—Å—ñ–≤\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# üîπ –°—Ç—Ä—ñ–º—ñ–Ω–≥ ‚Äî –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç –æ–¥—Ä–∞–∑—É\n",
    "ds = load_dataset(\"agentlans/high-quality-english-sentences\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f564fdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–æ–∫–µ–Ω—ñ–∑–æ–≤–∞–Ω–∏–π —Ç–µ–∫—Å—Ç: ['natural', 'language', 'processing', 'enables', 'machines', 'to', 'understand', 'human', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "# üîπ –í—Ö—ñ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç\n",
    "texts = \"Natural language processing enables machines to understand human language.\"\n",
    "\n",
    "# üîπ –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è\n",
    "tokens = word_tokenize(texts.lower())\n",
    "print(\"–¢–æ–∫–µ–Ω—ñ–∑–æ–≤–∞–Ω–∏–π —Ç–µ–∫—Å—Ç:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1907d41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî∏ –¢–æ–∫–µ–Ω–∏ –±–µ–∑ —Å—Ç–æ–ø-—Å–ª—ñ–≤:\n",
      "['natural', 'enables', 'understand', 'human']\n"
     ]
    }
   ],
   "source": [
    "# üîπ –ê–Ω–≥–ª—ñ–π—Å—å–∫—ñ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update([\"language\", \"processing\", \"machines\"])\n",
    "\n",
    "# üîπ –î–æ–¥–∞—Ç–∫–æ–≤—ñ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "my_stopwords = {\n",
    "    \"the\", \"is\", \"and\", \"to\", \"in\", \"this\", \"language\", \"processing\"\n",
    "}\n",
    "\n",
    "# üîπ –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è: –≤–∏–¥–∞–ª—è—î–º–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ —ñ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—é\n",
    "filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "print(\"üî∏ –¢–æ–∫–µ–Ω–∏ –±–µ–∑ —Å—Ç–æ–ø-—Å–ª—ñ–≤:\")\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f049f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_filtered_sentences = []\n",
    "\n",
    "for i, sample in enumerate(ds):\n",
    "    sentence = sample[\"text\"]\n",
    "    no_filtered_sentences.append(sentence)\n",
    "\n",
    "    if i >= 3000:  # ‚ö†Ô∏è –ù–∞–ø—Ä–∏–∫–ª–∞–¥, –æ–±—Ä–æ–±–∏—Ç–∏ –ª–∏—à–µ –ø–µ—Ä—à—ñ 10 —Ç–∏—Å—è—á —Ä–µ—á–µ–Ω—å\n",
    "        break\n",
    "\n",
    "# üîπ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —É —Ñ–∞–π–ª (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)\n",
    "with open(\"no_filtered_sentences.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in no_filtered_sentences:\n",
    "        f.write(sentence + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df26033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentences = []\n",
    "\n",
    "for i, sample in enumerate(ds):\n",
    "    sentence = sample[\"text\"]\n",
    "    tokens_ds = word_tokenize(sentence.lower())\n",
    "    filtered = [w for w in tokens_ds if w.isalpha() and w not in stop_words]\n",
    "    filtered_sentences.append(filtered)\n",
    "\n",
    "    if i >= 3000:  # ‚ö†Ô∏è –ù–∞–ø—Ä–∏–∫–ª–∞–¥, –æ–±—Ä–æ–±–∏—Ç–∏ –ª–∏—à–µ –ø–µ—Ä—à—ñ 10 —Ç–∏—Å—è—á —Ä–µ—á–µ–Ω—å\n",
    "        break\n",
    "\n",
    "# üîπ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —É —Ñ–∞–π–ª (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)\n",
    "with open(\"filtered_sentences.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in filtered_sentences:\n",
    "        f.write(\" \".join(sentence) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9e0c37",
   "metadata": {},
   "source": [
    "#### <center> <p style = 'background-color:darkblue'><b>Task - 2</p> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "07a4f323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í–µ–∫—Ç–æ—Ä –¥–ª—è —Å–ª–æ–≤–∞ 'human': [ 0.61854   0.11915  -0.46786   0.31368   1.0334    0.95964   0.87803\n",
      " -1.0346    1.6322    0.29347   0.80844  -0.058903  0.021251  0.40986\n",
      "  0.54443  -0.33311   0.53712  -0.35823   0.29374   0.090151 -0.92049\n",
      "  0.69386   0.39098  -0.64392   0.77831  -1.7215   -0.48393  -0.50327\n",
      " -0.22508   0.099192  3.2095   -0.31554  -0.71754  -1.6752   -1.3537\n",
      "  0.15195   0.054557 -0.1633   -0.027993  0.3917   -0.55007  -0.079205\n",
      "  0.63389   0.51446   0.70124   0.27638  -0.53445   0.064808 -0.21974\n",
      " -0.52048 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_glove_model(glove_file):\n",
    "    glove_model = {}\n",
    "    with open(glove_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float32)\n",
    "            glove_model[word] = embedding\n",
    "    return glove_model\n",
    "\n",
    "glove_file = \"glove.6B.50d.txt\"  # –ù–µ–æ–±—Ö—ñ–¥–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ñ–∞–π–ª –∑–∞–∑–¥–∞–ª–µ–≥—ñ–¥—å https://drive.google.com/file/d/1mN2Ee44z6CBkCW4QQ95gHHjQTaSk-fxM/view?usp=sharing\n",
    "glove_model = load_glove_model(glove_file)\n",
    "print(\"–í–µ–∫—Ç–æ—Ä –¥–ª—è —Å–ª–æ–≤–∞ 'human':\", glove_model.get(\"human\", \"–°–ª–æ–≤–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d9783b",
   "metadata": {},
   "source": [
    "#### <center> <p style = 'background-color:darkblue'><b>Task - 3</p> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "05ef4c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–æ—Å–∏–Ω—É—Å–Ω–∞ –ø–æ–¥—ñ–±–Ω—ñ—Å—Ç—å –º—ñ–∂ —Å–ª–æ–≤–∞–º–∏ 'question' —Ç–∞ 'answer': 0.9242\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "# –í–∏–±—ñ—Ä –¥–≤–æ—Ö —Å–ª—ñ–≤\n",
    "word1 = \"question\"\n",
    "word2 = \"answer\"\n",
    "\n",
    "vec1 = glove_model.get(word1)\n",
    "vec2 = glove_model.get(word2)\n",
    "\n",
    "if vec1 is not None and vec2 is not None:\n",
    "    similarity = cosine_similarity(vec1, vec2)\n",
    "    print(f\"–ö–æ—Å–∏–Ω—É—Å–Ω–∞ –ø–æ–¥—ñ–±–Ω—ñ—Å—Ç—å –º—ñ–∂ —Å–ª–æ–≤–∞–º–∏ '{word1}' —Ç–∞ '{word2}': {similarity:.4f}\")\n",
    "else:\n",
    "    print(\"–û–¥–Ω–µ –∞–±–æ –æ–±–∏–¥–≤–∞ —Å–ª–æ–≤–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω—ñ —É –º–æ–¥–µ–ª—ñ.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dbad27",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34d67c5d",
   "metadata": {},
   "source": [
    "#### <center> <p style = 'background-color:darkblue'><b>Task - 4</p> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ccabac4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°—Ö–æ–∂—ñ —Å–ª–æ–≤–∞ –¥–æ 'woman':\n",
      "girl: 0.9065\n",
      "man: 0.8860\n",
      "mother: 0.8764\n",
      "her: 0.8613\n",
      "boy: 0.8596\n",
      "she: 0.8431\n",
      "herself: 0.8225\n",
      "child: 0.8108\n",
      "wife: 0.8037\n",
      "old: 0.7982\n"
     ]
    }
   ],
   "source": [
    "def find_similar_words(glove_model, query_word, top_n=10):\n",
    "    if query_word not in glove_model:\n",
    "        print(f\"–°–ª–æ–≤–æ '{query_word}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤ –º–æ–¥–µ–ª—ñ.\")\n",
    "        return []\n",
    "\n",
    "    query_vector = glove_model[query_word]\n",
    "    similarities = {}\n",
    "\n",
    "    for word, vector in glove_model.items():\n",
    "        if word != query_word:\n",
    "            sim = cosine_similarity(query_vector, vector)\n",
    "            similarities[word] = sim\n",
    "\n",
    "    # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ –ø–æ–¥—ñ–±–Ω—ñ—Å—Ç—é —Ç–∞ –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ç–æ–ø-N\n",
    "    similar_words = sorted(similarities.items(), key=lambda item: item[1], reverse=True)[:top_n]\n",
    "\n",
    "    return similar_words\n",
    "\n",
    "# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:\n",
    "similar = find_similar_words(glove_model, \"woman\", top_n=10)\n",
    "print(\"–°—Ö–æ–∂—ñ —Å–ª–æ–≤–∞ –¥–æ 'woman':\")\n",
    "for word, sim in similar:\n",
    "    print(f\"{word}: {sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b91eb5",
   "metadata": {},
   "source": [
    "–ü—ñ–¥ —á–∞—Å –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ—ó —Ä–æ–±–æ—Ç–∏ –±—É–ª–æ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –ø—Ä–æ–≥—Ä–∞–º—É, —è–∫–∞ –≤–∏–∫–æ–Ω—É—î –æ—Å–Ω–æ–≤–Ω—ñ –µ—Ç–∞–ø–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ—ó –æ–±—Ä–æ–±–∫–∏ —Ç–µ–∫—Å—Ç—É: —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—é —Ç–∞ –≤–∏–¥–∞–ª–µ–Ω–Ω—è —Å—Ç–æ–ø-—Å–ª—ñ–≤ —ñ–∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏ nltk. –î–ª—è —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—ó –∑–∞—Å—Ç–æ—Å–æ–≤–∞–Ω–æ –º–µ—Ç–æ–¥ word_tokenize, –∞ –Ω–∞–±—ñ—Ä —Å—Ç–æ–ø-—Å–ª—ñ–≤ —Ä–æ–∑—à–∏—Ä–µ–Ω–æ –≤–ª–∞—Å–Ω–æ—Ä—É—á, —â–æ –¥–æ–∑–≤–æ–ª–∏–ª–æ –∫—Ä–∞—â–µ –≤—ñ–¥—Ñ—ñ–ª—å—Ç—Ä—É–≤–∞—Ç–∏ –∑–∞–π–≤—ñ —Å–ª–æ–≤–∞ –∑ —Ç–µ–∫—Å—Ç—É.\n",
    "\n",
    "–î–∞–Ω—ñ –±—É–ª–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ –∑ –∞–Ω–≥–ª–æ–º–æ–≤–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É \"high-quality-english-sentences\" —á–µ—Ä–µ–∑ –±—ñ–±–ª—ñ–æ—Ç–µ–∫—É datasets —É —Å—Ç—Ä—ñ–º—ñ–Ω–≥–æ–≤–æ–º—É —Ä–µ–∂–∏–º—ñ, —â–æ –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞–ª–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ—ó –ø–∞–º º—è—Ç—ñ.\n",
    "\n",
    "–ü—ñ—Å–ª—è –æ—á–∏—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –±—É–ª–æ —Å—Ñ–æ—Ä–º–æ–≤–∞–Ω–æ –∫–æ—Ä–ø—É—Å —Ä–µ—á–µ–Ω—å, –Ω–∞ –æ—Å–Ω–æ–≤—ñ —è–∫–æ–≥–æ –ø–æ–±—É–¥–æ–≤–∞–Ω–æ –≤–µ–∫—Ç–æ—Ä–Ω—ñ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è —Å–ª—ñ–≤ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –º–æ–¥–µ–ª—ñ GloVe (Global Vectors for Word Representation). –•–æ—á–∞ —É –∑–∞–≤–¥–∞–Ω–Ω—ñ –ø—Ä–æ–ø–æ–Ω—É–≤–∞–ª–æ—Å—å –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ Word2Vec, –æ–±—Ä–∞–Ω–æ GloVe ‚Äî —â–æ —î —Ü—ñ–ª–∫–æ–º –ø—Ä–∏–π–Ω—è—Ç–Ω–∏–º, –∞–¥–∂–µ —Ü—è –º–æ–¥–µ–ª—å —Ç–∞–∫–æ–∂ —î word-embedding –ø—ñ–¥—Ö–æ–¥–æ–º, –∞–ª–µ –∑ –≤—ñ–¥–º—ñ–Ω–Ω–∏–º –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–∏–º –ø—ñ–¥“ë—Ä—É–Ω—Ç—è–º.\n",
    "\n",
    "–ó–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –æ—Ç—Ä–∏–º–∞–Ω–∏—Ö –≤–µ–∫—Ç–æ—Ä—ñ–≤ –±—É–ª–æ —Ä–æ–∑—Ä–∞—Ö–æ–≤–∞–Ω–æ –∫–æ—Å–∏–Ω—É—Å–Ω—É –ø–æ–¥—ñ–±–Ω—ñ—Å—Ç—å –º—ñ–∂ –ø–∞—Ä–∞–º–∏ —Å–ª—ñ–≤, —â–æ –¥–∞–ª–æ –∑–º–æ–≥—É –æ—Ü—ñ–Ω–∏—Ç–∏ —ó—Ö–Ω—é —Å–µ–º–∞–Ω—Ç–∏—á–Ω—É –±–ª–∏–∑—å–∫—ñ—Å—Ç—å —É –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É –ø—Ä–æ—Å—Ç–æ—Ä—ñ. –¢–∞–∫–æ–∂ –∑–Ω–∞–π–¥–µ–Ω–æ —Å–ª–æ–≤–∞, —è–∫—ñ –Ω–∞–π–±—ñ–ª—å—à–µ —Å—Ö–æ–∂—ñ –Ω–∞ –∑–∞–¥–∞–Ω–µ, –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —Ñ—É–Ω–∫—Ü—ñ—ó –ø–æ—à—É–∫—É –Ω–∞–π–±–ª–∏–∂—á–∏—Ö –≤–µ–∫—Ç–æ—Ä—ñ–≤.\n",
    "\n",
    "–£ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è —Ä–æ–±–æ—Ç–∏ –æ—Ç—Ä–∏–º–∞–Ω–æ –ø—Ä–∞–∫—Ç–∏—á–Ω–∏–π –¥–æ—Å–≤—ñ–¥ —É:\n",
    "\n",
    "–ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –æ–±—Ä–æ–±—Ü—ñ —Ç–µ–∫—Å—Ç—É;\n",
    "\n",
    "—Ä–æ–±–æ—Ç—ñ –∑ –∫–æ—Ä–ø—É—Å–∞–º–∏ —á–µ—Ä–µ–∑ —Å—Ç—Ä—ñ–º—ñ–Ω–≥;\n",
    "\n",
    "–ø–æ–±—É–¥–æ–≤—ñ –≤–µ–∫—Ç–æ—Ä–Ω–∏—Ö —É—è–≤–ª–µ–Ω—å —Å–ª—ñ–≤;\n",
    "\n",
    "–ø–æ—à—É–∫—É —Å—Ö–æ–∂–∏—Ö —Å–ª—ñ–≤ —É embedding-–ø—Ä–æ—Å—Ç–æ—Ä—ñ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbf1727",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
